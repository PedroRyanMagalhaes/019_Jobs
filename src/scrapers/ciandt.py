from playwright.sync_api import sync_playwright, TimeoutError
import time
import random
import re
from config.settings import SCRAPER_CONFIG, LOCAIS_ALVO, EMPRESA_URLS

# --- CONSTANTES ---
USER_AGENT = SCRAPER_CONFIG["user_agent"]
URL_CIANDT = EMPRESA_URLS["ciandt"]
BASE_URL = "https://ciandt.com"

# --- FUN√á√ïES AUXILIARES ---
def human_delay(min_seconds=None, max_seconds=None):
    """Pausa a execu√ß√£o por um tempo aleat√≥rio para simular comportamento humano."""
    min_sec = min_seconds or SCRAPER_CONFIG["delay_min"]
    max_sec = max_seconds or SCRAPER_CONFIG["delay_max"]
    time.sleep(random.uniform(min_sec, max_sec))

def extrair_titulo_e_localizacao(texto_completo):
    """
    Fun√ß√£o inteligente para separar o t√≠tulo da localiza√ß√£o.
    Busca pelas palavras-chave de localiza√ß√£o (LOCAIS_ALVO) de tr√°s para frente.
    """
    texto_lower = texto_completo.lower()
    
    for local in LOCAIS_ALVO:
        posicao = texto_lower.rfind(local.lower())
        
        if posicao != -1:
            titulo = texto_completo[:posicao].strip()
            if titulo.endswith(',') or titulo.endswith('-'):
                titulo = titulo[:-1].strip()
            
            localizacao = texto_completo[posicao:].strip()
            return titulo, localizacao
            
    return texto_completo, "Local n√£o especificado"

# --- FUN√á√ÉO PRINCIPAL DO SCRAPER ---
def raspar():
    print("üöÄ Iniciando scraping da CI&T...")
    vagas_encontradas = []

    with sync_playwright() as p:
        browser = None
        try:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(user_agent=USER_AGENT, locale='pt-BR')
            page = context.new_page()
            
            print("Navegando para a p√°gina de carreiras da CI&T...")
            page.goto(URL_CIANDT, timeout=60000)
        
            # SELETORES MANTIDOS COMO O ORIGINAL
            try:
                print("Procurando por banner de cookies para declinar...")
                decline_button = page.locator('xpath=/html/body/div[2]/div/div/div[2]/button[2]')
                decline_button.click(timeout=5000)
                print("‚úÖ Banner de cookies declinado com sucesso.")
                human_delay()
            except TimeoutError:
                print("‚òëÔ∏è Banner de cookies n√£o encontrado ou n√£o vis√≠vel (o que √© normal).")

            try:
                print("Fechando botao OK")
                ok_button = page.locator('xpath=//*[@id="modalOkBtn"]')
                ok_button.click(timeout=5000)
                print("‚úÖ Botao OK fechado com sucesso.")
            except TimeoutError:
                print("‚òëÔ∏è Botao OK n√£o encontrado ou n√£o vis√≠vel (o que √© normal).")
            
            print("Filtrando por vagas no Brasil...")
            # CORRE√á√ÉO DE TYPO MANTIDA
            search_input = page.locator('#filter-by-text')
            search_input.fill("Brazil")
            search_input.press("Enter")
            page.wait_for_timeout(3000) 

            print("Buscando a lista de vagas...")
            # CORRE√á√ÉO DE TYPO MANTIDA
            vagas_items = page.locator('div.opprtunity-item').all()
            print(f"Encontrados {len(vagas_items)} itens de vagas na p√°gina.")

            if not vagas_items:
                print("Nenhuma vaga encontrada ap√≥s aplicar o filtro.")
                return []

            for item in vagas_items:
                try:
                    texto_completo_vaga = item.locator('h2').inner_text()
                    
                    # PASSO 1: CORRE√á√ÉO PRINCIPAL MANTIDA
                    titulo, localizacao = extrair_titulo_e_localizacao(texto_completo_vaga)

                    if localizacao == "Local n√£o especificado":
                        continue

                    link_relativo = item.locator('a').get_attribute('href')
                    url_vaga = f"{BASE_URL}{link_relativo}"
                    
                    # PASSO 4: REFATORA√á√ÉO DO MODELO DE TRABALHO MANTIDA
                    modelo_trabalho = "Nao informado"
                    try:
                        hidden_span_text = item.locator('span.sr-only.filters-item').inner_text(timeout=1000)
                        match = re.search(r'Workplace_type_(\w+)', hidden_span_text)
                        if match:
                            modelo_trabalho = match.group(1).replace("Remote", "Remoto")
                    except (TimeoutError, AttributeError):
                        pass

                    vaga = {
                        "empresa": "CI&T",
                        "titulo": titulo,
                        "modelo_trabalho": modelo_trabalho,
                        "localizacao": localizacao,
                        "url_vaga": url_vaga,
                    }
                    vagas_encontradas.append(vaga)
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao extrair dados de um item de vaga: {e}")
            
            print(f"Scraping da CI&T finalizado. Total de vagas v√°lidas para a regi√£o: {len(vagas_encontradas)}")

        except TimeoutError as e:
            print(f"‚ùå Erro de Timeout: A p√°gina demorou muito para carregar ou um elemento n√£o foi encontrado. {e}")
        except Exception as e:
            print(f"‚ùå Ocorreu um erro inesperado durante o scraping: {e}")
        finally:
            if browser and browser.is_connected():
                browser.close()
    
    return vagas_encontradas

# --- BLOCO DE TESTE ---
if __name__ == "__main__":
    print("--- MODO DE TESTE DO SCRAPER CI&T ---")
    vagas_coletadas = raspar()

    if vagas_coletadas:
        print(f"\n‚úÖ SUCESSO! Total de {len(vagas_coletadas)} vagas encontradas.")
        print("--- Exibindo as 5 primeiras vagas: ---")

        for i, vaga in enumerate(vagas_coletadas[:5]):
            print(f"\n--- VAGA {i+1} ---")
            print(vaga)
    else:
        print("\n‚ùå Nenhuma vaga foi encontrada durante o teste.")