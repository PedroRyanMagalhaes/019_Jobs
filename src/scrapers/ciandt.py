from playwright.sync_api import sync_playwright, TimeoutError
import time
import random
import re
import os
from datetime import datetime
from config.settings import SCRAPER_CONFIG, LOCAIS_ALVO, EMPRESA_URLS

# --- CONSTANTES ---
USER_AGENT = SCRAPER_CONFIG["user_agent"]
URL_CIANDT = EMPRESA_URLS["ciandt"]
BASE_URL = "https://ciandt.com"

# --- FUN√á√ïES AUXILIARES ---
def human_delay(min_seconds=None, max_seconds=None):
    """Pausa a execu√ß√£o por um tempo aleat√≥rio para simular comportamento humano."""
    min_sec = min_seconds or SCRAPER_CONFIG["delay_min"]
    max_sec = max_seconds or SCRAPER_CONFIG["delay_max"]
    time.sleep(random.uniform(min_sec, max_sec))

def salvar_vagas_descartadas(vagas_descartadas, nome_empresa="CI&T"):
    """Salva apenas as vagas que foram descartadas para verifica√ß√£o do filtro."""
    try:
        if not vagas_descartadas:
            print("‚úÖ Nenhuma vaga foi descartada - filtro funcionando perfeitamente!")
            return None
            
        # Criar diret√≥rio data se n√£o existir
        os.makedirs("data", exist_ok=True)
        
        # Nome do arquivo com timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        nome_arquivo = f"data/vagas_descartadas_{nome_empresa.lower().replace('&', 'e')}_{timestamp}.txt"
        
        with open(nome_arquivo, 'w', encoding='utf-8') as arquivo:
            arquivo.write(f"=== VAGAS DESCARTADAS - {nome_empresa} ===\n")
            arquivo.write(f"Data/Hora: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
            arquivo.write(f"Total de vagas descartadas: {len(vagas_descartadas)}\n")
            arquivo.write(f"Motivo: Localiza√ß√£o n√£o est√° em {LOCAIS_ALVO}\n")
            arquivo.write("="*60 + "\n\n")
            
            for i, vaga in enumerate(vagas_descartadas, 1):
                arquivo.write(f"VAGA DESCARTADA {i:03d}:\n")
                arquivo.write(f"  Texto Original: {vaga['texto_original']}\n")
                arquivo.write(f"  T√≠tulo Extra√≠do: {vaga['titulo']}\n")
                arquivo.write(f"  Localiza√ß√£o Extra√≠da: {vaga['localizacao']}\n")
                arquivo.write(f"  Motivo Descarte: {vaga['motivo']}\n")
                arquivo.write(f"  URL: {vaga['url_vaga']}\n")
                arquivo.write("-" * 40 + "\n\n")
        
        print(f"‚ö†Ô∏è {len(vagas_descartadas)} vagas foram descartadas - verifique o arquivo: {nome_arquivo}")
        return nome_arquivo
        
    except Exception as e:
        print(f"‚ùå Erro ao salvar arquivo de vagas descartadas: {e}")
        return None

def extrair_titulo_e_localizacao(texto_completo):
    """
    Fun√ß√£o inteligente para separar o t√≠tulo da localiza√ß√£o.
    Trata casos especiais como minorias e v√≠rgulas no t√≠tulo.
    """
    import re
    
    # Remove quebras de linha e espa√ßos extras
    texto_completo = ' '.join(texto_completo.split())
    
    # Padr√µes para remover informa√ß√µes sobre vagas afirmativas
    padroes_afirmativos = [
        r'\s*\(.*?afirmativ.*?\)',
        r'\s*\(.*?mulher.*?\)',
        r'\s*\(.*?pessoa.*?defici√™ncia.*?\)',
        r'\s*\(.*?pretas.*?\)',
        r'\s*\(.*?lgbtqiapn.*?\)',
        r'\s*\(.*?diversidade.*?\)',
        r'\s*\(.*?inclus.*?\)',
        r'\s*\|\s*vaga\s+afirmativa.*?(?=\s|$)',
        r'\s*-\s*afirmativa.*?(?=\s|$)',
        r'\s*\|\s*affirmative.*?(?=\s|$)',
        r'\s*exclusiva\s+pcd\s*-?\s*',
    ]
    
    # Remove padr√µes afirmativos do texto original
    texto_limpo = texto_completo
    for padrao in padroes_afirmativos:
        texto_limpo = re.sub(padrao, '', texto_limpo, flags=re.IGNORECASE)
    
    # Limpa espa√ßos extras ap√≥s remo√ß√£o
    texto_limpo = ' '.join(texto_limpo.split())
    
    # Procura por localiza√ß√µes conhecidas
    for local in LOCAIS_ALVO:
        # Cria padr√µes mais espec√≠ficos para encontrar a localiza√ß√£o
        padroes_local = [
            rf'\b{re.escape(local)}\b\s*$',  # Local no final
            rf'\s+{re.escape(local)}\b\s*$',  # Local no final com espa√ßo
            rf',\s*{re.escape(local)}\b\s*$',  # Local ap√≥s v√≠rgula no final
        ]
        
        for padrao in padroes_local:
            match = re.search(padrao, texto_limpo, re.IGNORECASE)
            if match:
                # Extrai o t√≠tulo (tudo antes da localiza√ß√£o)
                titulo = texto_limpo[:match.start()].strip()
                
                # Remove v√≠rgulas, h√≠fens ou pipes no final do t√≠tulo
                while titulo.endswith((',', '-', '|')):
                    titulo = titulo[:-1].strip()
                
                return titulo, local
    
    # Se n√£o encontrou localiza√ß√£o conhecida, tenta o padr√£o de v√≠rgula
    if ',' in texto_limpo:
        # Pega a √∫ltima v√≠rgula (mais prov√°vel de separar t√≠tulo de localiza√ß√£o)
        partes = texto_limpo.rsplit(',', 1)
        if len(partes) == 2:
            titulo_candidato = partes[0].strip()
            localizacao_candidata = partes[1].strip()
            
            # Verifica se a localiza√ß√£o candidata √© razo√°vel
            # (n√£o muito longa e n√£o cont√©m caracteres especiais t√≠picos de t√≠tulo)
            if (len(localizacao_candidata.split()) <= 3 and 
                len(localizacao_candidata) <= 50 and
                not re.search(r'[()[\]{}]', localizacao_candidata)):
                
                return titulo_candidato, localizacao_candidata
    
    # Se ainda n√£o encontrou, verifica se h√° alguma localiza√ß√£o no meio do texto
    for local in LOCAIS_ALVO:
        if local.lower() in texto_limpo.lower():
            # Encontra a posi√ß√£o e tenta extrair
            match = re.search(rf'\b{re.escape(local)}\b', texto_limpo, re.IGNORECASE)
            if match:
                titulo = texto_limpo[:match.start()].strip()
                while titulo.endswith((',', '-', '|')):
                    titulo = titulo[:-1].strip()
                return titulo, local
            
    return texto_completo, "Local n√£o especificado"

# --- FUN√á√ÉO PRINCIPAL DO SCRAPER ---
def raspar():
    print("üöÄ Iniciando scraping da CI&T...")
    vagas_encontradas = []
    vagas_descartadas = []


    with sync_playwright() as p:
        browser = None
        try:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context(user_agent=USER_AGENT, locale='pt-BR')
            page = context.new_page()
            
            print("Navegando para a p√°gina de carreiras da CI&T...")
            page.goto(URL_CIANDT, timeout=60000)
        
            # SELETORES MANTIDOS COMO O ORIGINAL
            try:
                print("Procurando por banner de cookies para declinar...")
                decline_button = page.locator('xpath=/html/body/div[2]/div/div/div[2]/button[2]')
                decline_button.click(timeout=5000)
                print("‚úÖ Banner de cookies declinado com sucesso.")
                human_delay()
            except TimeoutError:
                print("‚òëÔ∏è Banner de cookies n√£o encontrado ou n√£o vis√≠vel (o que √© normal).")

            try:
                print("Fechando botao OK")
                ok_button = page.locator('xpath=//*[@id="modalOkBtn"]')
                ok_button.click(timeout=5000)
                print("‚úÖ Botao OK fechado com sucesso.")
            except TimeoutError:
                print("‚òëÔ∏è Botao OK n√£o encontrado ou n√£o vis√≠vel (o que √© normal).")
            
            print("Filtrando por vagas no Brasil...")
            # CORRE√á√ÉO DE TYPO MANTIDA
            search_input = page.locator('#filter-by-text')
            search_input.fill("Brazil")
            search_input.press("Enter")
            page.wait_for_timeout(3000) 

            print("Buscando a lista de vagas...")
            # CORRE√á√ÉO DE TYPO MANTIDA
            vagas_items = page.locator('div.opprtunity-item').all()
            print(f"Encontrados {len(vagas_items)} itens de vagas na p√°gina.")

            if not vagas_items:
                print("Nenhuma vaga encontrada ap√≥s aplicar o filtro.")
                return []

            for item in vagas_items:
                try:
                    texto_completo_vaga = item.locator('h2').inner_text()
                    link_relativo = item.locator('a').get_attribute('href')
                    url_vaga = f"{BASE_URL}{link_relativo}"
                    
                    # PASSO 1: CORRE√á√ÉO PRINCIPAL MANTIDA
                    titulo, localizacao = extrair_titulo_e_localizacao(texto_completo_vaga)

                    # Verificar se a localiza√ß√£o est√° nos locais alvo
                    if localizacao not in LOCAIS_ALVO:
                        # Salvar vaga descartada para an√°lise
                        vaga_descartada = {
                            "texto_original": texto_completo_vaga,
                            "titulo": titulo,
                            "localizacao": localizacao,
                            "motivo": f"Localiza√ß√£o '{localizacao}' n√£o est√° em {LOCAIS_ALVO}",
                            "url_vaga": url_vaga
                        }
                        vagas_descartadas.append(vaga_descartada)
                        continue

                    # PASSO 4: REFATORA√á√ÉO DO MODELO DE TRABALHO MANTIDA
                    modelo_trabalho = "Nao informado"
                    try:
                        hidden_span_text = item.locator('span.sr-only.filters-item').inner_text(timeout=1000)
                        match = re.search(r'Workplace_type_(\w+)', hidden_span_text)
                        if match:
                            modelo_trabalho = match.group(1).replace("Remote", "Remoto")
                    except (TimeoutError, AttributeError):
                        pass

                    vaga = {
                        "empresa": "CI&T",
                        "titulo": titulo,
                        "modelo_trabalho": modelo_trabalho,
                        "localizacao": localizacao,
                        "url_vaga": url_vaga,
                    }
                    vagas_encontradas.append(vaga)
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao extrair dados de um item de vaga: {e}")
            
            print(f"Scraping da CI&T finalizado.")
            print(f"üìä Vagas v√°lidas para a regi√£o: {len(vagas_encontradas)}")
            print(f"üö´ Vagas descartadas: {len(vagas_descartadas)}")

        except TimeoutError as e:
            print(f"‚ùå Erro de Timeout: A p√°gina demorou muito para carregar ou um elemento n√£o foi encontrado. {e}")
        except Exception as e:
            print(f"‚ùå Ocorreu um erro inesperado durante o scraping: {e}")
        finally:
            if browser and browser.is_connected():
                browser.close()
    
    # Salvar arquivo TXT das vagas descartadas para verifica√ß√£o
    salvar_vagas_descartadas(vagas_descartadas, "CI&T")
    
    return vagas_encontradas

# --- BLOCO DE TESTE ---
if __name__ == "__main__":
    print("--- MODO DE TESTE DO SCRAPER CI&T ---")
    vagas_coletadas = raspar()

    if vagas_coletadas:
        print(f"\n‚úÖ SUCESSO! Total de {len(vagas_coletadas)} vagas encontradas.")
        print("--- Exibindo as 5 primeiras vagas: ---")

        for i, vaga in enumerate(vagas_coletadas[:5]):
            print(f"\n--- VAGA {i+1} ---")
            print(vaga)
    else:
        print("\n‚ùå Nenhuma vaga foi encontrada durante o teste.")