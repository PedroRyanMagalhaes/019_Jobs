from src.database import database
from src.scrapers import ciandt
from config.settings import SCRAPERS_ATIVOS

# Mapeamento de nomes para m√≥dulos
SCRAPERS_MAP = {
    "ciandt": ciandt
}

# Criar lista de scrapers a partir das configura√ß√µes
SCRAPERS_A_EXECUTAR = [
    (SCRAPERS_MAP[nome_modulo], nome_empresa) 
    for nome_modulo, nome_empresa in SCRAPERS_ATIVOS
]

def main():
    print("== INICIANDO PROCESSO DE COLETA DE VAGAS ==")
    print("==============================================")

    print("\n--- PASSO 1: Preparando o Banco de Dados ---")
    database.inicializar_banco()

    for scraper_module, nome_empresa in SCRAPERS_A_EXECUTAR:
        print(f"\n--- PASSO: Executando Scraper da {nome_empresa} ---")
        vagas_coletadas = []
        try:
            # Padroniza√ß√£o: todo m√≥dulo de scraper deve ter uma fun√ß√£o chamada 'raspar()'
            vagas_coletadas = scraper_module.raspar()
        except Exception as e:
            print(f"‚ùå Falha cr√≠tica ao executar o scraper da {nome_empresa}: {e}")

        if vagas_coletadas:
            print(f"Processando {len(vagas_coletadas)} vagas encontradas na {nome_empresa}...")
            novas_vagas_salvas = 0
            for vaga in vagas_coletadas:
                if database.salvar_vaga(vaga):
                    novas_vagas_salvas += 1
            print(f"\nResumo {nome_empresa}: {novas_vagas_salvas} novas vagas foram salvas.")
        else:
            print(f"Nenhuma vaga nova da {nome_empresa} para salvar.")

    print("\n==============================================")
    print("== PROCESSO FINALIZADO ==")
    print("üìÑ Verifique a pasta 'data/' para arquivos de vagas descartadas!")


if __name__ == "__main__":
    main()